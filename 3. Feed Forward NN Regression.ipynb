{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58304df",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1fae4b",
   "metadata": {},
   "source": [
    "# Feed Forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689a199f",
   "metadata": {},
   "source": [
    "## A non-linear Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7debc",
   "metadata": {},
   "source": [
    "Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e70c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, 10, 100);\n",
    "y = (x.-2).^2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d8be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870584a7",
   "metadata": {},
   "source": [
    "### Implementing Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baade8e",
   "metadata": {},
   "source": [
    "If we adapted the method from before we could fit a linear regression to it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f8aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "function model(m, b, xv)\n",
    "    return m * x .+ b\n",
    "end\n",
    "\n",
    "m = rand()\n",
    "b = rand()\n",
    "yhat = model(m, b, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f5e658",
   "metadata": {},
   "source": [
    "This model is random though, so it's not a very good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe49ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, y)\n",
    "plot!(x, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7cdfb6",
   "metadata": {},
   "source": [
    "If we implement our gradient, loss and optimiser functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d812887",
   "metadata": {},
   "outputs": [],
   "source": [
    "function mgrad(x, y, m, b)\n",
    "    grad = sum(-2 *x .* (y - m * x .+ b))\n",
    "    return grad\n",
    "end\n",
    "\n",
    "function bgrad(x, y, m, b)\n",
    "    grad = sum(2 * (y - m * x .+ b))\n",
    "    return grad\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b701e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss(y, yhat) = sum((y-yhat).^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a75314",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00001\n",
    "function opt(mg, bg, m, b, lr)\n",
    "    m -= lr * mg\n",
    "    b -= lr * bg\n",
    "\n",
    "    return m,b\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78052f8",
   "metadata": {},
   "source": [
    "We can train our model with a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa0aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "EPOCHS = 100\n",
    "\n",
    "for i=1:100\n",
    "    # Fit the model\n",
    "    yhat = model(m, b, x)\n",
    "    \n",
    "    # Measure the loss\n",
    "    loss = rss(yhat, y)\n",
    "    append!(losses, loss)\n",
    "    \n",
    "    # Calculate the gradients\n",
    "    mg = mgrad(x, y, m, b)\n",
    "    bg = bgrad(x, y, m, b)\n",
    "    \n",
    "    # Backpropogate\n",
    "    m, b = opt(mg, bg, m, b, lr)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc2aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model(m, b, x)\n",
    "plot(x, yhat)\n",
    "plot!(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b9a10e",
   "metadata": {},
   "source": [
    "Unfourtunately, a linear model **will not work** for a quadratic function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75498e09",
   "metadata": {},
   "source": [
    "### A non-linear fix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead5423",
   "metadata": {},
   "source": [
    "So instead we change our model, instead of just doing linear regression, we round it off, and then do linear regression on the result!\n",
    "\n",
    "There are a few choices for how we round it off, we can do a literal `round()` (but then differentiation doesn't work well because of all the 0s), we can use a sigmoid, or we can use a compromise that's become pretty common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "function relu(x)\n",
    "    if x > 0\n",
    "        return x\n",
    "    else\n",
    "        return 0\n",
    "    end\n",
    "end\n",
    "\n",
    "# Derivative of relu is step\n",
    "function drelu(x)\n",
    "    if x > 0\n",
    "        return 1\n",
    "    else\n",
    "        return 0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(relu,   label = \"Relu\")\n",
    "plot!(drelu, label = \"Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d5cadd",
   "metadata": {},
   "source": [
    "We can use this *activation function* and it's derivative with our model and use the the chain rule to get the derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a07e96c",
   "metadata": {},
   "source": [
    "Let's consider our new model (the matrix sizes have been annotated):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b2066",
   "metadata": {},
   "source": [
    "$$\n",
    "\\underset{1\\times N}{\\underbrace{\\mathbf{\\hat{Y}}}}=\\underset{1\\times3}{\\underbrace{\\mathbf{A}}}\\overset{3\\times N}{\\overbrace{{\\rm relu}\\left(\\underset{3\\times1}{\\underbrace{\\mathbf{B}}}\\underset{1\\times N}{\\underbrace{\\mathbf{X}}}\\right)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaecc664",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sigmoid(x)\n",
    "    1/(1+exp(x))\n",
    "end\n",
    "\n",
    "function dsig(x)\n",
    "    -exp(x)/(exp(x)+1)^2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d295db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu(x) = sin(x)\n",
    "# drelu(x) = cos(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f70599",
   "metadata": {},
   "source": [
    "This can be expressed in julia like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c344d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, 10, 100);\n",
    "y = x.^2;\n",
    "\n",
    "x = Matrix(reshape(x, (1, :)))\n",
    "y = Matrix(reshape(y, (1, :)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac5c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(1, 4)\n",
    "B = rand(4, 1)\n",
    "\n",
    "function nn(A, B, x)\n",
    "    return (A * relu.(B*x))\n",
    "end\n",
    "yhat = nn(A, B, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809495b",
   "metadata": {},
   "source": [
    "Note that x,y have been transposed such that each column is an observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc629a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(vec(x), vec(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d30898",
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO this is wrong, test it in R:\n",
    "\n",
    "\n",
    "```r\n",
    "A = matrix(runif(3), 1, 6)\n",
    "B = matrix(runif(3), 6, 1)\n",
    " relu <- function(x) {\n",
    "        x*(x>0)\n",
    "    }\n",
    "nn <-function(A, B, x) {\n",
    "        return (A %*% relu(B %*% x))\n",
    "    }\n",
    "\n",
    "x    = seq(from = -5, to = 5, length.out = 50)\n",
    "yhat = nn(A, B, seq(from = -5, to = 5, length.out = 50))\n",
    "\n",
    "plot(x, yhat)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c10ddc",
   "metadata": {},
   "source": [
    "### A Detour into Matrices and Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686037d2",
   "metadata": {},
   "source": [
    "Unfourtunately we now have to deal with vector calculus, which is awful. Consider this example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ce1d2",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "{\\bf T} & ={\\bf W}{\\bf X}\\\\\n",
    "{\\bf T}_{\\left[i,j\\right]} & =\\sum_{k=1}^{{\\tt ncol}\\left(\\mathbf{W}\\right)}\\left[{\\bf W}_{\\left[i,k\\right]}{\\bf X}_{\\left[k,j\\right]}\\right]\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcc2691",
   "metadata": {},
   "source": [
    "#### 4D Tensors into Matrices\n",
    "\n",
    "The first thing to note are these two identities, assuming $\\mathbf{X}$ contains observations.\n",
    "\n",
    "If each column is an observation (like in Julia):\n",
    "\n",
    "$$\n",
    "p = q \\implies \\frac{\\partial{\\bf T}_{pi}}{\\partial{\\bf W}_{qj}} = 0\n",
    "$$\n",
    "\n",
    "For this reason, the 4D gradient tensor can be reduced to a 2D tensor and a convenient notation is used where we just *pretend* the gradient tensor is 2D:\n",
    "\n",
    "$$\n",
    "\\left(\\frac{\\partial{\\bf T}}{\\partial{\\bf W}}\\right)_{\\left[i,j\\right]}=\\frac{\\partial{\\bf T}_{qi}}{\\partial{\\bf W}_{qj}}\n",
    "$$\n",
    "\n",
    "\n",
    "In the *Matrix Cookbook* this is expressed as:\n",
    "\n",
    "$$\n",
    "\\left(\\frac{\\partial{\\bf T}}{\\partial{\\bf W}}\\right)_{\\left[i,j\\right]}=\\frac{\\partial{\\bf T}_{pi}}{\\partial{\\bf W}_{qj}}\\mathbf{I}_{pq}\n",
    "$$\n",
    "\n",
    "That way the identity matrix zeros out when they're not equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd80aa63",
   "metadata": {},
   "source": [
    "#### Solving the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a4c08",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "{\\bf T} & ={\\bf W}{\\bf X}\\\\\n",
    "{\\bf T}_{ij} & =\\sum_{k=1}^{{\\tt ncol}\\left(\\mathbf{W}\\right)}\\left[{\\bf W}_{ik}{\\bf X}_{kj}\\right]\\\\\n",
    "\\left(\\frac{\\partial{\\bf T}}{\\partial{\\bf W}}\\right)_{\\left[i,j\\right]}=\\frac{\\partial{\\bf T}_{qi}}{\\partial{\\bf W}_{qj}} & =\\frac{\\partial}{\\partial{\\bf W}_{qj}}\\left(\\sum_{k=1}^{N}\\left[{\\bf W}_{qk}{\\bf X}_{kj}\\right]\\right)\\\\\n",
    " & =\\underset{k<q}{\\underbrace{0+0+0+\\cdots}}+\\frac{\\partial}{\\partial{\\bf W}_{qj}}\\left({\\bf W}_{qj}{\\bf X}_{ji}\\right)+\\underset{k>q}{\\underbrace{0+0+0+\\cdots}}\\\\\n",
    " & =\\frac{\\partial}{\\partial{\\bf W}_{qj}}\\left({\\bf W}_{qj}{\\bf X}_{ji}\\right)\\\\\n",
    " & ={\\bf X}_{ji}\\\\\n",
    "\\implies\\frac{\\partial{\\bf T}}{\\partial{\\bf W}} & ={\\bf X}^{{\\rm T}}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4ce99",
   "metadata": {},
   "source": [
    "Similarly for the gradient of the observations:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial{\\bf T}_{qi}}{\\partial{\\bf X}_{qj}} & =\\frac{\\partial}{\\partial{\\bf X}_{qj}}\\left(\\sum_{k=1}^{{\\tt nrow}\\left({\\bf W}\\right)}\\left[{\\bf W}_{ik}{\\bf X}_{kj}\\right]\\right)\\\\\n",
    " & =\\frac{\\partial}{\\partial{\\bf X}_{jq}}\\left({\\bf W}_{ij}{\\bf X}_{jq}\\right)\\\\\n",
    " &= {\\bf W}_{ij}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b3194",
   "metadata": {},
   "source": [
    "#### Transpose for Row Major"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62b0b94",
   "metadata": {},
   "source": [
    "You can transpose all this to get the same result for row-major:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f29653",
   "metadata": {},
   "source": [
    "If each row is an observation (like in python):\n",
    "\n",
    "$$\n",
    "p = q \\implies \\frac{\\partial{\\bf T}_{ip}}{\\partial{\\bf W}_{jq}} = 0\n",
    "$$\n",
    "\n",
    "And the tensor is expressed as:\n",
    "\n",
    "$$\n",
    "\\left(\\frac{\\partial{\\bf T}}{\\partial{\\bf W}}\\right)_{\\left[i,j\\right]}=\\frac{\\partial{\\bf T}_{ip}}{\\partial{\\bf W}_{jp}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e46d57",
   "metadata": {},
   "source": [
    "### TODO Using the Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1876cb8",
   "metadata": {},
   "source": [
    "Now we can take our model:\n",
    "\n",
    "$$\n",
    "\\underset{1\\times N}{\\underbrace{\\mathbf{\\hat{Y}}}}=\\underset{1\\times3}{\\underbrace{\\mathbf{A}}}\\overset{3\\times N}{\\overbrace{{\\rm relu}\\left(\\underset{3\\times1}{\\underbrace{\\mathbf{B}}}\\underset{1\\times N}{\\underbrace{\\mathbf{X}}}\\right)}}\n",
    "$$\n",
    "\n",
    "and solve:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79288d1",
   "metadata": {},
   "source": [
    "TODO show why the transpose checks out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32394a0c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial\\varepsilon}{\\partial{\\bf A}}=\\frac{\\partial\\hat{y}}{\\partial{\\bf A}}\\left(\\frac{\\partial\\varepsilon}{\\partial\\hat{y}}\\right)^{{\\rm T}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5465ded7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial\\varepsilon}{\\partial{\\bf B}}=\\frac{\\partial\\varepsilon}{\\partial\\hat{y}}\\frac{\\partial\\hat{y}}{\\partial{\\bf A}}\\frac{\\partial{\\bf A}}{\\partial{\\rm relu}}\\frac{\\partial{\\rm relu}}{\\partial{\\bf \\left({\\bf B}{\\bf X}\\right)}}\\frac{\\partial{\\bf \\left({\\bf B}{\\bf X}\\right)}}{\\partial{\\bf B}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667daaa",
   "metadata": {},
   "source": [
    "Now efore we go any further, the data has been reshaped so that each observation is a column and features are rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab44cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, 10, 100);\n",
    "y = x.^2;\n",
    "\n",
    "x = x'\n",
    "y = y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea00a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "function Agrad(x, A, B, y, yhat)\n",
    "    # Beware, this transpose makes this vector an 1XN matrix\n",
    "    # it's not the transpose in the math\n",
    "   \n",
    "    dedy = 2(yhat-y)\n",
    "    dyda = relu.(B*x)\n",
    "    \n",
    "    return dedy * dyda'\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d97d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Agrad(x, A, B, y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee52f7",
   "metadata": {},
   "source": [
    "In an ideal world we would do something like this:\n",
    "\n",
    "$$\n",
    "\\frac{{\\rm d}\\varepsilon}{{\\rm d}B}=\\frac{{\\rm d}\\varepsilon}{{\\rm d}\\hat{y}}\\frac{{\\rm d}\\hat{y}}{{\\rm d}{\\rm relu}}\\frac{{\\rm d}{\\rm relu}}{{\\rm d}\\left(BX\\right)}\\frac{{\\rm d}\\left(BX\\right)}{{\\rm d}B}\n",
    "$$\n",
    "\n",
    "but we have to do that transpose bullshit, so instead it helps to break it down:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial\\varepsilon}{\\partial\\mathbf{B}} & =\\frac{\\partial{\\rm relu}}{\\partial{\\bf B}}\\left(\\frac{\\partial\\varepsilon}{\\partial{\\rm relu}}\\right)^{{\\rm T}}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Solving those seperately:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial\\varepsilon}{\\partial{\\rm relu}} & =\\frac{\\partial\\hat{y}}{\\partial{\\rm relu}}\\left(\\frac{\\partial\\varepsilon}{\\partial\\hat{y}}\\right)^{{\\rm T}}\\\\\n",
    "\\frac{\\partial{\\rm relu}}{\\partial{\\bf B}} & =\\frac{\\partial\\left({\\bf B}{\\bf X}\\right)}{\\partial{\\bf B}}\\left(\\frac{\\partial{\\rm relu}}{\\partial\\left({\\bf B}{\\bf X}\\right)}\\right)^{{\\rm T}}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Putting it all together:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c3b41",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial\\hat{y}}{\\partial{\\rm relu}}={\\bf A}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094d41da",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial{\\bf B}}\\left({\\rm relu}\\left({\\bf B}{\\bf X}\\right)\\right)={\\bf B}\\left({\\rm step}\\left({\\bf B}{\\bf X}\\right)\\right)^{{\\rm T}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "function Bgrad(x, A, B, y, yhat)\n",
    "    # Beware, this transpose makes this vector an 1XN matrix\n",
    "    # it's not the transpose in the math\n",
    "   \n",
    "    \n",
    "    # Calculate error to relu\n",
    "    dydr = A\n",
    "    dedy = 2(yhat-y)\n",
    "    dedr = dydr' * dedy\n",
    "    \n",
    "    # Calculate relu to B\n",
    "    drdb = drelu.(B*x)' * B\n",
    "    \n",
    "    # Get final gradiant    \n",
    "    dedb = dedr * drdb\n",
    "    \n",
    "    return dedb\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dff283",
   "metadata": {},
   "outputs": [],
   "source": [
    "function opt(mg, bg, m, b, lr)\n",
    "    m -= lr * mg\n",
    "    b -= lr * bg\n",
    "\n",
    "    return m,b\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e1b7a",
   "metadata": {},
   "source": [
    "Now we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d8c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "EPOCHS = 100\n",
    "lr = 0.1\n",
    "A = rand(1, 3)*0.03\n",
    "B = rand(3, 1)*5\n",
    "for i=1:1000\n",
    "    # Fit the model\n",
    "    yhat = nn(A, B, x)\n",
    "    \n",
    "    # Measure the loss\n",
    "    loss = rss(yhat, y)\n",
    "    append!(losses, loss)\n",
    "    \n",
    "    # Calculate the gradients\n",
    "    Ag = Agrad(x, A, B, y, yhat)\n",
    "    Bg = Bgrad(x, A, B, y, yhat)\n",
    "    \n",
    "    # Backpropogate\n",
    "    A, B = opt(Ag, Bg, A, B, lr)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b766ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862129a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model(A, B, x)\n",
    "plot(x', vec(yhat))\n",
    "plot!(x', y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dddffce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f910e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
